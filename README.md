# Gamatrain AI - Educational LLM with RAG ğŸ¤–

Fine-tuned LLM (Qwen2.5-7B) with RAG-powered API for Gamatrain's educational platform.

## ğŸ¯ Overview

An AI assistant that:
- Answers questions about Gamatrain's educational content (courses, tests, blogs)
- Uses RAG (Retrieval-Augmented Generation) for accurate, context-aware responses
- Maintains conversation memory for follow-up questions
- Prevents hallucination with similarity threshold checks

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Fine-tuned LLM** | Qwen2.5-7B trained on Gamatrain content |
| **RAG Integration** | LlamaIndex-powered retrieval from 2000+ blogs |
| **Anti-Hallucination** | Similarity threshold + entity verification |
| **Conversation Memory** | Remembers context for follow-up questions |
| **Educational Format** | Structured teaching responses with concept explanation, examples, and comprehension checks |
| **OpenAI-Compatible API** | Drop-in replacement for OpenAI endpoints |
| **Multi-Provider** | Supports Ollama (local), Groq, OpenRouter |

## ğŸ“Š Model Stats

| Metric | Value |
|--------|-------|
| Base Model | Qwen2.5-7B-Instruct |
| Training Dataset | 2,614 samples |
| Domain Data | 2,422 (Gamatrain blogs, tests, courses) |
| General Data | 192 (math, logic, chat - weighted 4x) |
| Output Format | GGUF (4-bit quantized) |
| RAG Test Pass Rate | 92.9% |

## ğŸ—‚ï¸ Project Structure

```
gamatrain-ai-research/
â”œâ”€â”€ api/                           # API Server
â”‚   â”œâ”€â”€ llm_server.py              # Development server (Ollama)
â”‚   â”œâ”€â”€ llm_server_production.py   # Production server (Groq/OpenRouter)
â”‚   â”œâ”€â”€ requirements.txt           # Development dependencies
â”‚   â”œâ”€â”€ requirements-production.txt # Production dependencies
â”‚   â””â”€â”€ .env.production.example    # Environment template
â”œâ”€â”€ data/                          # Training & RAG Data
â”‚   â”œâ”€â”€ custom_docs.json           # Custom RAG documents
â”‚   â”œâ”€â”€ gamatrain_final_dataset.jsonl # Final training dataset
â”‚   â”œâ”€â”€ gamatrain_finetune_data.jsonl # Fine-tuning data
â”‚   â”œâ”€â”€ general_knowledge.jsonl    # General knowledge samples
â”‚   â””â”€â”€ scripts/                   # Data processing scripts
â”œâ”€â”€ model/
â”‚   â””â”€â”€ Modelfile                  # Ollama model configuration
â”œâ”€â”€ scripts/                       # Testing & Utility Scripts
â”‚   â”œâ”€â”€ test_model_and_rag.py      # Main test suite
â”‚   â”œâ”€â”€ test_random_blogs.py       # Random blog RAG tests
â”‚   â””â”€â”€ rebuild_index.py           # Index rebuilding utility
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ fine-tuning-complete.ipynb # Training notebook (Colab)
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md              # Basic deployment guide
â”‚   â”œâ”€â”€ PRODUCTION.md              # Production deployment guide
â”‚   â”œâ”€â”€ RESEARCH.md                # Research findings
â”‚   â””â”€â”€ TRAINING.md                # Fine-tuning guide
â”œâ”€â”€ storage/                       # RAG Index Storage
â”‚   â”œâ”€â”€ faiss_index.bin           # FAISS vector index
â”‚   â”œâ”€â”€ documents.json            # Document store
â”‚   â””â”€â”€ metadata.json             # Index metadata
â”œâ”€â”€ docker-compose.production.yml  # Production Docker setup
â””â”€â”€ Dockerfile.production          # Production Docker image
```


## ğŸš€ Quick Start

### Option 1: Local Development (with Ollama)

```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Import the fine-tuned model
cd model/
# Place qwen2.5-gamatrain.gguf here (see model/README.md)
ollama create gamatrain-qwen -f Modelfile

# 3. Start the API server
cd api/
pip install -r requirements.txt
python llm_server.py
# Server runs on http://localhost:8000
```

### Option 2: Production (No GPU Required)

Uses cloud LLM providers (Groq is free and fast).

```bash
# 1. Setup environment
cd api/
cp .env.production.example .env
# Edit .env and add your GROQ_API_KEY (free at https://console.groq.com)

# 2. Install and run
pip install -r requirements-production.txt
python llm_server_production.py
# Server runs on http://localhost:8001
```

### Option 3: Docker

```bash
docker-compose -f docker-compose.production.yml up -d
```

## ğŸ”Œ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/v1/query` | POST | RAG query with streaming |
| `/v1/chat/completions` | POST | OpenAI-compatible chat |
| `/v1/refresh` | POST | Refresh RAG index |
| `/v1/session/{id}` | DELETE | Clear conversation memory |
| `/health` | GET | Health check |

### Example Requests

```bash
# Simple query
curl -X POST http://localhost:8000/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Gamatrain?", "session_id": "user1"}'

# Follow-up question (uses conversation memory)
curl -X POST http://localhost:8000/v1/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Tell me more about that", "session_id": "user1"}'

# Refresh RAG index (after new content is added)
curl -X POST http://localhost:8000/v1/refresh
```

### Response Format

```json
{
  "query": "What is Gamatrain?",
  "response": "Gamatrain is an educational technology company...",
  "confidence": "high",
  "similarity_score": 0.897,
  "session_id": "user1"
}
```

## âš™ï¸ Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `PROVIDER` | ollama | LLM provider: `ollama`, `groq`, `openrouter` |
| `GROQ_API_KEY` | - | Groq API key (free tier available) |
| `GROQ_MODEL` | llama-3.1-8b-instant | Model to use with Groq |
| `OLLAMA_MODEL` | gamatrain-qwen | Local Ollama model name (Qwen2.5-7B based) |
| `SIMILARITY_THRESHOLD` | 0.45 | RAG confidence threshold |
| `MAX_TOKENS` | 1024 | Maximum response tokens |
| `PORT` | 8000/8001 | Server port |

## ğŸ›¡ï¸ Anti-Hallucination

The system prevents made-up responses through:

1. **Similarity Threshold** - Low-confidence queries return "I don't know"
2. **Entity Verification** - Checks if mentioned entities exist in context
3. **Strict Prompting** - Instructs model to only use provided context

## ğŸ§ª Running Tests

```bash
# Main test suite (RAG + Model)
python scripts/test_model_and_rag.py

# Random blog RAG tests
python scripts/test_random_blogs.py
```

## ğŸ“š Documentation

- [PRODUCTION.md](docs/PRODUCTION.md) - **Production deployment guide** (recommended)
- [DEPLOYMENT.md](docs/DEPLOYMENT.md) - Basic deployment guide
- [TRAINING.md](docs/TRAINING.md) - Fine-tuning guide
- [RESEARCH.md](docs/RESEARCH.md) - Research findings

## ğŸ“ˆ Version History

### v1.1 (Current - Commit 87b2d3e)
- âœ… **Model Upgrade**: Upgraded to Qwen2.5-7B-Instruct for better performance
- âœ… **Enhanced Teaching Format**: Structured educational response format
- âœ… Fine-tuned model with 2,614 training samples
- âœ… RAG system with 2000+ blog documents
- âœ… Conversation memory and follow-up question handling
- âœ… Anti-hallucination with similarity thresholds
- âœ… Multi-provider support (Ollama, Groq, OpenRouter)
- âœ… Comprehensive test suite with 92.9% pass rate
- âœ… Production-ready Docker deployment

### v1.0 (Previous)
- âœ… Initial release with Qwen2-1.5B model
- âœ… Basic RAG and conversation memory

### v2.0 (In Development)
- ğŸš§ Modular architecture with separate components
- ğŸš§ Source citation and automatic linking
- ğŸš§ Response regeneration endpoint
- ğŸš§ Enhanced RAG techniques
- ğŸš§ Extended test coverage

## âš ï¸ Key Learning: Catastrophic Forgetting

Fine-tuning only on domain data caused the model to "forget" basic abilities.

**Solution:** Mix domain data with general knowledge samples (weighted 4x).

| Before | After |
|--------|-------|
| `2 + 2 = 0` âŒ | `2 + 2 = 4` âœ… |

## ğŸ“„ License

MIT License
